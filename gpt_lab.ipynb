{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.0066e-01, -6.3600e-01,  4.1748e-02,  ..., -6.0802e-01,\n",
       "           1.7435e-01, -2.7198e-02],\n",
       "         [ 6.5966e-01, -9.9477e-01,  1.0112e-01,  ..., -7.8291e-01,\n",
       "          -3.0049e-02, -3.5142e-01],\n",
       "         [ 3.8858e-01, -4.8320e-01,  2.9340e-02,  ..., -4.7311e-01,\n",
       "           1.5257e-01, -1.5589e-04],\n",
       "         ...,\n",
       "         [ 2.4082e-01, -3.3719e-01,  2.9280e-02,  ..., -2.8882e-01,\n",
       "           3.2035e-02, -7.6046e-02],\n",
       "         [ 1.2454e-01, -1.6791e-01,  1.3239e-02,  ..., -1.5011e-01,\n",
       "           2.7284e-02, -2.6306e-02],\n",
       "         [ 3.1544e-01, -4.0989e-01,  2.9004e-02,  ..., -3.8202e-01,\n",
       "           9.4638e-02, -3.5619e-02]],\n",
       "\n",
       "        [[ 6.6103e-01, -9.9216e-01,  9.9953e-02,  ..., -7.8507e-01,\n",
       "          -2.2384e-02, -3.4276e-01],\n",
       "         [ 3.2419e-01, -4.7322e-01,  4.5087e-02,  ..., -3.8658e-01,\n",
       "           1.1185e-02, -1.4118e-01],\n",
       "         [ 8.7523e-01, -1.3788e+00,  1.5149e-01,  ..., -1.0319e+00,\n",
       "          -1.3749e-01, -5.8486e-01],\n",
       "         ...,\n",
       "         [ 5.4210e-01, -8.2243e-01,  8.4549e-02,  ..., -6.4280e-01,\n",
       "          -3.2891e-02, -2.9875e-01],\n",
       "         [ 7.2906e-01, -1.0246e+00,  8.9747e-02,  ..., -8.7395e-01,\n",
       "           9.0762e-02, -2.3778e-01],\n",
       "         [ 7.0074e-01, -1.0198e+00,  9.6543e-02,  ..., -8.3595e-01,\n",
       "           2.9314e-02, -2.9892e-01]],\n",
       "\n",
       "        [[-5.4754e-01,  6.1984e-01, -2.3392e-02,  ...,  6.7375e-01,\n",
       "          -3.1612e-01, -1.2264e-01],\n",
       "         [-3.1842e-01,  5.3357e-01, -6.4509e-02,  ...,  3.7171e-01,\n",
       "           1.0296e-01,  2.7709e-01],\n",
       "         [-3.2459e-01,  5.3997e-01, -6.4601e-02,  ...,  3.7937e-01,\n",
       "           9.8429e-02,  2.7453e-01],\n",
       "         ...,\n",
       "         [-6.9334e-01,  1.0676e+00, -1.1276e-01,  ...,  8.2031e-01,\n",
       "           6.8092e-02,  4.1371e-01],\n",
       "         [-5.2773e-01,  7.2851e-01, -6.1099e-02,  ...,  6.3415e-01,\n",
       "          -8.7478e-02,  1.4566e-01],\n",
       "         [-5.5550e-01,  7.8297e-01, -6.9055e-02,  ...,  6.6564e-01,\n",
       "          -6.5367e-02,  1.8578e-01]],\n",
       "\n",
       "        [[ 2.9923e-01, -7.6658e-01,  1.3860e-01,  ..., -3.1850e-01,\n",
       "          -5.3608e-01, -7.9411e-01],\n",
       "         [ 2.6027e-01, -5.9850e-01,  1.0048e-01,  ..., -2.8496e-01,\n",
       "          -3.5320e-01, -5.5333e-01],\n",
       "         [ 1.5205e-01, -3.4367e-01,  5.6944e-02,  ..., -1.6716e-01,\n",
       "          -1.9644e-01, -3.1123e-01],\n",
       "         ...,\n",
       "         [ 3.9441e-01, -7.2072e-01,  9.7497e-02,  ..., -4.5346e-01,\n",
       "          -2.2665e-01, -4.6363e-01],\n",
       "         [ 3.3164e-01, -5.7633e-01,  7.3248e-02,  ..., -3.8474e-01,\n",
       "          -1.4138e-01, -3.3008e-01],\n",
       "         [ 1.7915e-01, -2.5203e-01,  2.2129e-02,  ..., -2.1472e-01,\n",
       "           2.1875e-02, -5.8950e-02]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, num_features, num_steps,head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(num_features, head_size, bias=False)\n",
    "        self.key = nn.Linear(num_features, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_features, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "        # tensor that aren't parameters\n",
    "        self.register_buffer('tril_mask', torch.tril(torch.ones((num_steps, num_steps))))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # (B, T, H) - H=head_size\n",
    "        k = self.key(x) # (B, T, H)\n",
    "        v = self.value(x) # (B, T, H)\n",
    "        # affinities inter tokens (and scale)\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, T)\n",
    "        # mask future tokens\n",
    "        wei = wei.masked_fill(self.tril_mask[:T, :T] == 0, float('-inf'))\n",
    "        # normalize each row(each token interactions in last dim)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        y = wei @ v # apply affinities (B, T, H) weighted agreggation\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = torch.randn(4, 7, 2)\n",
    "B, T, C = X.shape\n",
    "\n",
    "att = AttentionHead(C, T,64)\n",
    "\n",
    "att(X)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
