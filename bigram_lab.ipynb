{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Corpus from https://www.corpusdoportugues.org/web-dial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "torch.manual_seed(7)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class CharLevelTokenizer():\n",
    "    def __init__(self,):\n",
    "        self.OOV = -1\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        # remove special chars\n",
    "        corpus = re.sub(r'[^a-zA-Z\\s\\sàáâãäèéêëìíîïòóôõöùúûüçß]' , '', corpus)\n",
    "        vocab = sorted(set(corpus))\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        # mapping \n",
    "        self.char_to_int = {c:i  for i,c in enumerate(vocab)}\n",
    "        self.int_to_char = {i:c  for i,c in enumerate(vocab)}\n",
    "        return corpus\n",
    "    \n",
    "    def encode(self, txt): # dont threat OOV \n",
    "        return torch.tensor([self.char_to_int[c] for c in txt], dtype=torch.long)\n",
    "    \n",
    "    def decode(self, token_seq):\n",
    "        token_seq = token_seq.to('cpu').numpy()\n",
    "        return ''.join([self.int_to_char[i] for i in token_seq])\n",
    "\n",
    "def get_batch(batch_size, block_size, from_train: bool):\n",
    "    data = train_corpus if from_train else val_corpus\n",
    "    # get random start batches indexes (one per batch)\n",
    "    idx_start = torch.randint(len(train_corpus)-block_size, (batch_size,))\n",
    "    # get each batch and stack them\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx_start])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx_start])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self,num_embeddings, emb_dim):\n",
    "        super().__init__()\n",
    "        # maps each token integer in a vector of emb_dim dimensions\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings, emb_dim)\n",
    "    \n",
    "    def forward(self, token_seq):\n",
    "        logits = self.token_embedding_table(token_seq)\n",
    "        # produces a score for each other token, indicating the chance of it be the next\n",
    "        return logits \n",
    "\n",
    "    def compute_loss(self, real_y, pred_y_logits):\n",
    "        # logits = isn't normalized to probabilities\n",
    "        # real_y contais the token index. logits contains one score per vocab possibility\n",
    "        B, T, C = pred_y_logits.shape\n",
    "        loss = F.cross_entropy(pred_y_logits.view(B*T, C), real_y.view(-1))# itsnot batch_first, need spreedout feature dim\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(self, idx, next_steps): # generate for each batch\n",
    "        self.eval()\n",
    "        idx = idx.clone()\n",
    "        # idx (B, T) is the array of token indexes of current history/context\n",
    "        for i in range(next_steps):\n",
    "            # print(idx[0])\n",
    "            last_step_logits = self(idx)[:, -1, :] # (B, T, vocab_size)\n",
    "            probs = F.softmax(last_step_logits, dim=-1) # (B, vocab_size) normalized logits = probability of each token be the next\n",
    "            # sample from probability\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # get 1 token_id per batch (B, 1)\n",
    "            # concatenate in each batch along the time dimension\n",
    "            idx = torch.concat((idx, idx_next), dim=1)\n",
    "        self.train()\n",
    "        return idx\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        for i in range(100):# 100 batches\n",
    "            batch_X, batch_y = get_batch(batch_size, block_size, True)\n",
    "            loss = self.compute_loss(batch_y, self(batch_X)).item()\n",
    "            losses.append(loss)\n",
    "        self.train()\n",
    "        return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "\n",
    "#\n",
    "# Data\n",
    "#\n",
    "corpus = open('text.txt','r').readlines()\n",
    "print(f'There are {len(corpus)} lines of PT raw text')\n",
    "corpus = ' '.join(corpus)\n",
    "print(f'There are {len(corpus)} chars')\n",
    "tokenizer = CharLevelTokenizer()\n",
    "corpus = tokenizer.fit_transform(corpus)\n",
    "print(f'There are {tokenizer.vocab_size} chars (without special chars)-> {tokenizer.vocab}')\n",
    "\n",
    "\n",
    "# 80% train\n",
    "train_corpus = tokenizer.encode(corpus[:int(len(corpus)*0.8)])\n",
    "val_corpus = tokenizer.encode(corpus[int(len(corpus)*0.8):])\n",
    "print(f'Train corpus:      {len(train_corpus)} tokens')\n",
    "print(f'Validation corpus: {len(val_corpus)} tokens')\n",
    "torch.manual_seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train Config\n",
    "#\n",
    "epochs = 20_000\n",
    "batch_size = 512\n",
    "block_size = 17\n",
    "lr = 1e-3\n",
    "model = BigramModel(tokenizer.vocab_size, tokenizer.vocab_size).to(device)\n",
    "#\n",
    "# Train\n",
    "#\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "for step in range(epochs):\n",
    "    X, y = get_batch(batch_size, block_size, True)\n",
    "    pred_y = model(X)\n",
    "    loss = model.compute_loss(y, pred_y)\n",
    "    #\n",
    "    optimizer.zero_grad() # current batch zero-out the loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f'Batch {step+1}/{epochs}-> Estimated loss: {model.estimate_loss()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "new_tokens = 1000\n",
    "prompt = 'O homem está indo até a '\n",
    "print(f'Generation for prompt \"{prompt}\":')\n",
    "prompt = tokenizer.encode(prompt).unsqueeze(0).to(device) # add batch dimension (1, T)\n",
    "gen = model.generate(prompt, new_tokens)[0]\n",
    "\n",
    "print('--> ', tokenizer.decode(gen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
