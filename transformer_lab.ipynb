{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Corpus from https://www.corpusdoportugues.org/web-dial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "torch.manual_seed(7)\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelTokenizer():\n",
    "    def __init__(self,):\n",
    "        self.OOV = -1\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        # remove special chars\n",
    "        corpus = re.sub(r'[^a-zA-Z\\s\\sàáâãäèéêëìíîïòóôõöùúûüçß]' , '', corpus)\n",
    "        vocab = sorted(set(corpus))\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        # mapping \n",
    "        self.char_to_int = {c:i  for i,c in enumerate(vocab)}\n",
    "        self.int_to_char = {i:c  for i,c in enumerate(vocab)}\n",
    "        return corpus\n",
    "    \n",
    "    def encode(self, txt): # dont threat OOV \n",
    "        return torch.tensor([self.char_to_int[c] for c in txt], dtype=torch.long)\n",
    "    \n",
    "    def decode(self, token_seq):\n",
    "        token_seq = token_seq.to('cpu').numpy()\n",
    "        return ''.join([self.int_to_char[i] for i in token_seq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10864 lines of PT raw text\n",
      "There are 57077729 chars\n",
      "There are 78 chars (without special chars)-> ['\\n', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ò', 'ó', 'ô', 'õ', 'ö', 'ù', 'ú', 'û', 'ü']\n",
      "Train corpus:      43551404 tokens\n",
      "Validation corpus: 10887851 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc727fe22b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = open('text.txt','r').readlines()\n",
    "print(f'There are {len(corpus)} lines of PT raw text')\n",
    "corpus = ' '.join(corpus)\n",
    "print(f'There are {len(corpus)} chars')\n",
    "tokenizer = CharLevelTokenizer()\n",
    "corpus = tokenizer.fit_transform(corpus)\n",
    "print(f'There are {tokenizer.vocab_size} chars (without special chars)-> {tokenizer.vocab}')\n",
    "\n",
    "\n",
    "# 80% train\n",
    "train_corpus = tokenizer.encode(corpus[:int(len(corpus)*0.8)])\n",
    "val_corpus = tokenizer.encode(corpus[int(len(corpus)*0.8):])\n",
    "print(f'Train corpus:      {len(train_corpus)} tokens')\n",
    "print(f'Validation corpus: {len(val_corpus)} tokens')\n",
    "torch.manual_seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, block_size, from_train: bool):\n",
    "    data = train_corpus if from_train else val_corpus\n",
    "    # get random start batches indexes (one per batch)\n",
    "    idx_start = torch.randint(len(train_corpus)-block_size, (batch_size,))\n",
    "    # get each batch and stack them\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx_start])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx_start])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout faz com que, aleatoriamente, alguns dos neuronios nao propaguem sua informacao\n",
    "dessa forma, no momento de treinamento ẽ como se fizesemos umaamostragemd as subredes possiveis com a arquitetura completa\n",
    "no momento de inferencia (desligando o dropout), é coo se tivessemos um emsenble de subredes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, num_features, num_steps, head_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(num_features, head_size, bias=False)\n",
    "        self.key = nn.Linear(num_features, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_features, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "        # tensor that aren't parameters\n",
    "        self.register_buffer('tril_mask', torch.tril(torch.ones((num_steps, num_steps))))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # (B, T, H) - H=head_size\n",
    "        k = self.key(x) # (B, T, H)\n",
    "        v = self.value(x) # (B, T, H)\n",
    "        # affinities inter tokens (and scale)\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, T)\n",
    "        # mask future tokens\n",
    "        wei = wei.masked_fill(self.tril_mask[:T, :T] == 0, float('-inf'))\n",
    "        # normalize each row(each token interactions in last dim)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei) # dropout some of the affinities\n",
    "        y = wei @ v # apply affinities (B, T, H) weighted agreggation\n",
    "        return y\n",
    "X = torch.randn(4, 7, 2)\n",
    "B, T, C = X.shape\n",
    "att = AttentionHead(C, T,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embed, num_steps, head_size, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(n_embed, num_steps, head_size) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.concat([h(x) for h in self.heads], dim=-1)   \n",
    "        x = self.linear(x)# projection to back from residual path\n",
    "        x = self.dropout(x)\n",
    "        return  x    \n",
    "#\n",
    "att = MultiHeadAttention(n_embed=64, num_steps=7, head_size= 64//2, num_heads=2)\n",
    "att(torch.randn(4, 7, 64)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> Dropout pode ser adicionado na volta das conexoes residuais (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1588,  0.3787, -0.8805,  ...,  1.0510,  0.4706, -1.0371],\n",
       "         [-1.0621, -0.0123, -1.9748,  ...,  1.7488,  0.3916, -0.3095],\n",
       "         [-0.3462,  1.4477, -1.0345,  ...,  0.8921, -0.6130, -1.4711],\n",
       "         ...,\n",
       "         [-1.1539, -0.9808, -0.0541,  ..., -0.0959, -0.7640, -1.1543],\n",
       "         [ 1.2088,  1.9731,  0.7214,  ...,  2.4247,  1.2140,  1.4205],\n",
       "         [ 0.3872,  0.4355,  2.2424,  ...,  1.5555, -0.5274, -1.2757]],\n",
       "\n",
       "        [[ 0.0784,  1.5401,  0.8719,  ...,  1.1819, -0.2078,  0.8841],\n",
       "         [-0.5080,  0.0797,  0.7205,  ..., -0.1810,  0.8352, -0.4017],\n",
       "         [ 1.1117, -0.6180, -1.6750,  ...,  0.5678, -1.8029, -0.9104],\n",
       "         ...,\n",
       "         [-0.4415,  1.6644,  0.3497,  ..., -1.3769, -0.0759, -0.8147],\n",
       "         [ 0.6626, -0.1242, -0.2670,  ...,  0.8746, -1.2493, -0.0885],\n",
       "         [ 0.1783,  0.1121, -0.9020,  ..., -0.1184, -0.2558,  1.2034]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout=0.2):\n",
    "        super().__init__()\n",
    "        feed_forward_dim = n_embed * 4 # can be a hyperparameter (its a projection)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, feed_forward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feed_forward_dim, n_embed),# projection to back to residual path\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, num_steps, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.self_attention = MultiHeadAttention(n_embed, num_steps, head_size, num_heads)\n",
    "        self.feedforward = FeedForward(n_embed)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embed) # per token normalization\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    # input shape = (B, T, emb_size) --> output: (B, T, emb_size)\n",
    "    def forward(self, x):\n",
    "        # version 1\n",
    "        # x = self.self_attention(x) \n",
    "        # x = self.feedforward(x)\n",
    "        # version 2\n",
    "        # x = x + self.self_attention(x)  # residual connections\n",
    "        # x = x + self.feedforward(x)   # residual connections\n",
    "        # version 3 (different from original paper\n",
    "        # now layernorm is more commont to be applied before attention)\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))  # residual connections\n",
    "        x = x + self.feedforward(self.layer_norm_2(x))   # residual connections\n",
    "        return x\n",
    "\n",
    "att = TransformerBlock(64, 10, 2)\n",
    "att(torch.randn(2, 10, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 78])\n",
      "Num of weights: 5365582\n"
     ]
    }
   ],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        # Set model vars\n",
    "        expected_vars = ['vocab_size','block_size','emb_dim','num_heads','n_layers']\n",
    "        for v in expected_vars:\n",
    "            assert v in model_params.keys(), f'Key \"{v}\" is missing on params dict'\n",
    "            vars(self)[v] = model_params[v]\n",
    "        #\n",
    "        assert (self.emb_dim % self.num_heads == 0), 'emb_dim must be divisible by num_heads'\n",
    "        # maps each token integer in a vector of emb_dim dimensions\n",
    "        # learnable embeddings\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(self.block_size, self.emb_dim) # positional embeddig (for each position from 0 to blocksize-1 it will return an embedding(different vector))\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(self.emb_dim, self.block_size, self.num_heads) for _ in range(self.n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(self.emb_dim) # model final layer norm\n",
    "        \n",
    "        # self.mh_attention = MultiHeadAttention(self.emb_dim, self.block_size, self.head_size//self.num_heads, self.num_heads)\n",
    "        self.feedforward = FeedForward(self.emb_dim)\n",
    "        self.dense = nn.Linear(self.emb_dim, self.vocab_size) # final scores\n",
    "    \n",
    "    def forward(self, token_seq):\n",
    "        token_emb = self.token_emb(token_seq) # (B, T) --> (B, T, Emb)\n",
    "        pos_emb = self.pos_emb(torch.arange(self.block_size).to(device))# (B, T) --> (B, T, Emb)\n",
    "        x = token_emb + pos_emb # # concat embeddings x is holds token value(identity, embeddings from value) and positional information of this token (encoded as numbers(embs..))\n",
    "        # x = self.mh_attention(x) # feature extraction outputs (B, T, head_size)\n",
    "        # # computation\n",
    "        # # x = self.feedforward(x) # outputs (B, T, head_size)\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.dense(x) # outputs (B, T, vocab_size)\n",
    "        # produces a score for each other token, indicating the chance of it be the next\n",
    "        return logits \n",
    "\n",
    "    def compute_loss(self, real_y, pred_y_logits):\n",
    "        # logits = isn't normalized to probabilities\n",
    "        # real_y contais the token index. logits contains one score per vocab possibility\n",
    "        B, T, C = pred_y_logits.shape\n",
    "        loss = F.cross_entropy(pred_y_logits.view(B*T, C), real_y.view(-1))# itsnot batch_first, need spreedout feature dim\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(self, idx, next_steps): # generate for each batch\n",
    "        self.eval()\n",
    "        idx = idx.clone()\n",
    "        # idx (B, T) is the array of token indexes of current history/context\n",
    "        for i in range(next_steps):\n",
    "            # print(idx[0])\n",
    "            logits = self(idx[:, -self.block_size:])\n",
    "            logits = logits[:, -1, :] # (B, T, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size) normalized logits = probability of each token be the next\n",
    "            # sample from probability\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # get 1 token_id per batch (B, 1)\n",
    "            # concatenate in each batch along the time dimension\n",
    "            idx = torch.concat((idx, idx_next), dim=1)\n",
    "        self.train()\n",
    "        return idx\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        for i in range(100):# 100 batches\n",
    "            batch_X, batch_y = get_batch(batch_size, self.block_size, True)\n",
    "            loss = self.compute_loss(batch_y, self(batch_X)).item()\n",
    "            losses.append(loss)\n",
    "        self.train()\n",
    "        return np.mean(losses)\n",
    "#\n",
    "# Train Config\n",
    "#\n",
    "batch_size = 32\n",
    "block_size = 256\n",
    "model_params = {\n",
    "    'block_size': block_size,\n",
    "    'emb_dim': 256,\n",
    "    'num_heads': 8,\n",
    "    'n_layers':6, # number of stacked attention+computation blocks\n",
    "    'vocab_size':tokenizer.vocab_size,\n",
    "}\n",
    "model = AttentionModel(model_params).to(device)\n",
    "X, y = get_batch(4, block_size, from_train=True)\n",
    "# print(model)\n",
    "pred = model(X)\n",
    "print(pred.shape)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Num of weights:',pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/1000-> Estimated loss: 2.5900693273544313\n",
      "Batch 21/1000-> Estimated loss: 2.4776795148849486\n",
      "Batch 41/1000-> Estimated loss: 2.4356844449043273\n",
      "Batch 61/1000-> Estimated loss: 2.4150777316093444\n",
      "Batch 81/1000-> Estimated loss: 2.4057177114486694\n",
      "Batch 101/1000-> Estimated loss: 2.39168523311615\n",
      "Batch 121/1000-> Estimated loss: 2.3812510776519775\n",
      "Batch 141/1000-> Estimated loss: 2.3755004525184633\n",
      "Batch 161/1000-> Estimated loss: 2.3687994956970213\n",
      "Batch 181/1000-> Estimated loss: 2.365294795036316\n",
      "Batch 201/1000-> Estimated loss: 2.3600674867630005\n",
      "Batch 221/1000-> Estimated loss: 2.3596919703483583\n",
      "Batch 241/1000-> Estimated loss: 2.354325897693634\n",
      "Batch 261/1000-> Estimated loss: 2.3484859108924865\n",
      "Batch 281/1000-> Estimated loss: 2.348948585987091\n",
      "Batch 301/1000-> Estimated loss: 2.3458184576034546\n",
      "Batch 321/1000-> Estimated loss: 2.3417710971832277\n",
      "Batch 341/1000-> Estimated loss: 2.3439987897872925\n",
      "Batch 361/1000-> Estimated loss: 2.3399182963371277\n",
      "Batch 381/1000-> Estimated loss: 2.335361695289612\n",
      "Batch 401/1000-> Estimated loss: 2.3339939856529237\n",
      "Batch 421/1000-> Estimated loss: 2.331912784576416\n",
      "Batch 441/1000-> Estimated loss: 2.321932861804962\n",
      "Batch 461/1000-> Estimated loss: 2.3188439321517946\n",
      "Batch 481/1000-> Estimated loss: 2.3155942010879516\n",
      "Batch 501/1000-> Estimated loss: 2.3117396664619445\n",
      "Batch 521/1000-> Estimated loss: 2.3016035318374635\n",
      "Batch 541/1000-> Estimated loss: 2.292803182601929\n",
      "Batch 561/1000-> Estimated loss: 2.2833289074897767\n",
      "Batch 581/1000-> Estimated loss: 2.2749291849136353\n",
      "Batch 601/1000-> Estimated loss: 2.261349232196808\n",
      "Batch 621/1000-> Estimated loss: 2.238887536525726\n",
      "Batch 641/1000-> Estimated loss: 2.2285712337493897\n",
      "Batch 661/1000-> Estimated loss: 2.2175448751449585\n",
      "Batch 681/1000-> Estimated loss: 2.196860375404358\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# current batch zero-out the loss\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for step in range(epochs):\n",
    "    X, y = get_batch(batch_size, block_size, True)\n",
    "    pred_y = model(X)\n",
    "    loss = model.compute_loss(y, pred_y)\n",
    "    #\n",
    "    optimizer.zero_grad() # current batch zero-out the loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 20 == 0:\n",
    "        print(f'Batch {step+1}/{epochs}-> Estimated loss: {model.estimate_loss()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2340843670.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    logits = model(idx[:, -block\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def generate(model, idx, next_steps): # generate for each batch\n",
    "        model.eval()\n",
    "        idx = idx.clone()\n",
    "        # idx (B, T) is the array of token indexes of current history/context\n",
    "        for i in range(next_steps):\n",
    "            # print(idx[0])\n",
    "            logits = model(idx[:, -block_size:])\n",
    "            logits = logits[:, -1, :] # (B, T, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size) normalized logits = probability of each token be the next\n",
    "            # sample from probability\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # get 1 token_id per batch (B, 1)\n",
    "            # concatenate in each batch along the time dimension\n",
    "            idx = torch.concat((idx, idx_next), dim=1)\n",
    "        model.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->  ossas ilhas em busca de algo para mostrarem em outras paragens como troféus de as suas fúrias gananciosas e potenciadas por as favoráveis abanadelas de as nossas árvores autóctones  Felizmente para nós  temos uma artista cheia de propósitos a emitir contin\n",
      "-->  ossas ilhas em busca de algo para mostrarem em outras paragens como troféus de as suas fúrias gananciosas e potenciadas por as favoráveis abanadelas de as nossas árvores autóctones  Felizmente para nós  temos uma artista cheia de propósitos a emitir continicrátino  am der a cando a posera Byoncidoge cinoiadende óse u eoam fom TAnhomo  m Dicistltoralho de\n"
     ]
    }
   ],
   "source": [
    "new_tokens = 100\n",
    "prompt, _ = get_batch(1, block_size, from_train=True)\n",
    "gen = generate(model, prompt, new_tokens)[0]\n",
    "print('--> ', tokenizer.decode(prompt[0]))\n",
    "print('--> ', tokenizer.decode(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation for prompt \"Todas as pessoas gostam de comer coisas gostodas em restaurante\":\n",
      "torch.Size([1, 63])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (63) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# add batch dimension (1, T)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--> \u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mdecode(gen))\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, idx, next_steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# idx (B, T) is the array of token indexes of current history/context\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(next_steps):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(idx[0])\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, vocab_size) normalized logits = probability of each token be the next\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m, in \u001b[0;36mAttentionModel.forward\u001b[0;34m(self, token_seq)\u001b[0m\n\u001b[1;32m     24\u001b[0m token_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_emb(token_seq) \u001b[38;5;66;03m# (B, T) --> (B, T, Emb)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size)\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;66;03m# (B, T) --> (B, T, Emb)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_emb\u001b[49m \u001b[38;5;66;03m# # concat embeddings x is holds token value(identity, embeddings from value) and positional information of this token (encoded as numbers(embs..))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# x = self.mh_attention(x) # feature extraction outputs (B, T, head_size)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# # computation\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# # x = self.feedforward(x) # outputs (B, T, head_size)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (63) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "new_tokens = 100\n",
    "prompt = 'Todas as pessoas gostam de comer coisas gostodas em restaurante'\n",
    "print(f'Generation for prompt \"{prompt}\":')\n",
    "prompt = tokenizer.encode(prompt).unsqueeze(0).to(device) # add batch dimension (1, T)\n",
    "print(prompt.shape)\n",
    "gen = generate(model, prompt, new_tokens)[0]\n",
    "\n",
    "print('--> ', tokenizer.decode(gen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
